{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# *Lux*: Iterative Optimization for Linear Models\n",
    "\n",
    "This tutorial demonstrates the {py:func}`~pollux.Lux.optimize_iterative` method, which provides an efficient alternating optimization agenda for *Lux* models with linear transforms. This method exploits the linear structure of the model to solve sub-problems exactly using weighted least squares, often converging faster than gradient-based optimization with Adam (the default for {py:func}`~pollux.Lux.optimize`).\n",
    "\n",
    "This tutorial builds on the [Getting Started tutorial](Lux-linear-simulated-data.ipynb), so we'll skip the basic introductions and jump straight to comparing the two optimization approaches.\n",
    "\n",
    "We'll start with the standard imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import numpyro\n",
    "from helpers import make_simulated_linear_data\n",
    "\n",
    "import pollux as plx\n",
    "from pollux.models import LinearTransform\n",
    "\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## Generating simulated data\n",
    "\n",
    "We'll generate the same simulated data as in the Getting Started tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_stars = 2048\n",
    "n_latents = 8\n",
    "n_labels = 2\n",
    "n_flux = 128\n",
    "\n",
    "rng = np.random.default_rng(seed=8675309)\n",
    "\n",
    "A = np.zeros((n_labels, n_latents))\n",
    "A[0, 0] = 1.0\n",
    "A[1, 1] = 1.0\n",
    "\n",
    "B = rng.normal(scale=0.1, size=(n_flux, n_latents))\n",
    "B[:, 0] = B[:, 0] + 4 * np.exp(-0.5 * (np.arange(n_flux) - n_flux / 2) ** 2 / 5**2)\n",
    "B[:, 1] = B[:, 1] + 2 * np.exp(-0.5 * (np.arange(n_flux) - n_flux / 4) ** 2 / 3**2)\n",
    "\n",
    "data, truth = make_simulated_linear_data(\n",
    "    n_stars=n_stars,\n",
    "    n_latents=n_latents,\n",
    "    n_flux=n_flux,\n",
    "    n_labels=n_labels,\n",
    "    A=A,\n",
    "    B=B,\n",
    "    rng=rng,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "Package the data with preprocessors and create train/test splits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = plx.data.PolluxData(\n",
    "    flux=plx.data.OutputData(\n",
    "        data[\"flux\"],\n",
    "        err=data[\"flux_err\"],\n",
    "        preprocessor=plx.data.ShiftScalePreprocessor.from_data(data[\"flux\"]),\n",
    "    ),\n",
    "    label=plx.data.OutputData(\n",
    "        data[\"label\"],\n",
    "        err=data[\"label_err\"],\n",
    "        preprocessor=plx.data.ShiftScalePreprocessor.from_data(data[\"label\"]),\n",
    "    ),\n",
    ")\n",
    "\n",
    "preprocessed_data = all_data.preprocess()\n",
    "train_data = preprocessed_data[: n_stars // 2]\n",
    "test_data = preprocessed_data[n_stars // 2 :]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## Setting up the model\n",
    "\n",
    "We create a *Lux* model with two linear outputs, exactly as in the Getting Started tutorial:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = plx.LuxModel(latent_size=n_latents)\n",
    "model.register_output(\"label\", LinearTransform(output_size=n_labels))\n",
    "model.register_output(\"flux\", LinearTransform(output_size=n_flux))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "## Comparing optimization methods\n",
    "\n",
    "Now we'll compare the standard `optimize()` method with the new `optimize_iterative()` function.\n",
    "\n",
    "### Standard optimization with `optimize()`\n",
    "\n",
    "The standard approach uses gradient-based optimization (SVI with Adam) to jointly optimize all parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "opt_pars_svi, svi_results = model.optimize(\n",
    "    train_data,\n",
    "    rng_key=jax.random.PRNGKey(112358),\n",
    "    optimizer=numpyro.optim.Adam(1e-3),\n",
    "    num_steps=10_000,\n",
    "    svi_run_kwargs={\"progress_bar\": False},\n",
    ")\n",
    "svi_results.losses.block_until_ready()\n",
    "svi_time = time.time() - t0\n",
    "print(f\"SVI optimization time: {svi_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "### Iterative optimization with `optimize_iterative()`\n",
    "\n",
    "The iterative approach exploits the linear structure of the model. For linear transforms like `y = A @ z`, the optimal latents (z) given A, and the optimal A given z, can each be solved exactly using weighted least squares. The algorithm alternates between these two steps:\n",
    "\n",
    "1. **Fix output parameters, solve for latents**: Given the current A matrices, solve for optimal z using least squares\n",
    "2. **Fix latents, solve for output parameters**: Given the current z, solve for optimal A matrices using least squares\n",
    "\n",
    "This is repeated until convergence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = time.time()\n",
    "iterative_result = model.optimize_iterative(\n",
    "    train_data,\n",
    "    max_cycles=50,\n",
    "    tol=1e-6,\n",
    "    rng_key=jax.random.PRNGKey(112358),\n",
    "    progress=False,\n",
    ")\n",
    "iterative_time = time.time() - t0\n",
    "print(f\"Iterative optimization time: {iterative_time:.2f} seconds\")\n",
    "print(\n",
    "    f\"Converged: {iterative_result.converged} after {iterative_result.n_cycles} cycles\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Comparing convergence\n",
    "\n",
    "Let's visualize how the loss evolves for both methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4), layout=\"constrained\")\n",
    "\n",
    "# SVI loss trajectory\n",
    "axes[0].semilogy(svi_results.losses)\n",
    "axes[0].set(xlabel=\"Step\", ylabel=\"Loss\", title=\"SVI Optimization\")\n",
    "axes[0].axhline(\n",
    "    svi_results.losses[-1],\n",
    "    color=\"tab:orange\",\n",
    "    ls=\"--\",\n",
    "    label=f\"Final: {svi_results.losses[-1]:.1f}\",\n",
    ")\n",
    "axes[0].legend()\n",
    "\n",
    "# Iterative loss trajectory\n",
    "axes[1].semilogy(iterative_result.losses_per_cycle)\n",
    "axes[1].set(xlabel=\"Cycle\", ylabel=\"Loss\", title=\"Iterative Optimization\")\n",
    "axes[1].axhline(\n",
    "    iterative_result.losses_per_cycle[-1],\n",
    "    color=\"tab:orange\",\n",
    "    ls=\"--\",\n",
    "    label=f\"Final: {iterative_result.losses_per_cycle[-1]:.1f}\",\n",
    ")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.suptitle(\"Convergence Comparison\", fontsize=14)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "The iterative method typically converges in far fewer iterations (cycles) than SVI requires steps, and each cycle involves closed-form solutions rather than gradient computations.\n",
    "\n",
    "## Evaluating on the test set\n",
    "\n",
    "To evaluate the model on unseen data, we need to infer the latents for the test set while keeping the learned model parameters (A matrices) fixed. We do this by passing `fixed_pars` to both optimization methods. For the iterative method, we also pass the fixed parameters as `initial_params`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fixed_pars containing the trained model parameters (everything except latents)\n",
    "fixed_pars_svi = {k: v for k, v in opt_pars_svi.items() if k != \"latents\"}\n",
    "\n",
    "# Create test data with only flux (we want to predict labels using only flux)\n",
    "test_flux_only = plx.data.PolluxData(flux=test_data[\"flux\"])\n",
    "\n",
    "# Optimize latents for test set using SVI (fix model parameters)\n",
    "# Use names=[\"flux\"] to only model the flux output (since we don't have label data)\n",
    "test_pars_svi, _ = model.optimize(\n",
    "    test_flux_only,\n",
    "    rng_key=jax.random.PRNGKey(42),\n",
    "    optimizer=numpyro.optim.Adam(1e-3),\n",
    "    num_steps=2000,\n",
    "    fixed_pars=fixed_pars_svi,\n",
    "    names=[\"flux\"],\n",
    "    svi_run_kwargs={\"progress_bar\": False},\n",
    ")\n",
    "# Merge the fixed parameters back with the optimized latents\n",
    "test_pars_svi = {**fixed_pars_svi, **test_pars_svi}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "Now we do the same for the iterative method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pollux.models.iterative import ParameterBlock\n",
    "\n",
    "opt_pars_iter = iterative_result.params\n",
    "fixed_pars_iter = {k: v for k, v in opt_pars_iter.items() if k != \"latents\"}\n",
    "\n",
    "# Optimize latents for test set using iterative method\n",
    "# Only optimize latents (not output parameters) since we're using fixed trained params\n",
    "# Pass initial_params with the trained A matrices and zero latents\n",
    "initial_test_pars = {\n",
    "    **fixed_pars_iter,\n",
    "    \"latents\": jnp.zeros((len(test_flux_only), model.latent_size)),\n",
    "}\n",
    "test_blocks = [ParameterBlock(\"latents\", \"latents\", optimizer=\"least_squares\")]\n",
    "test_result_iter = model.optimize_iterative(\n",
    "    test_flux_only,\n",
    "    blocks=test_blocks,\n",
    "    max_cycles=50,\n",
    "    tol=1e-6,\n",
    "    initial_params=initial_test_pars,\n",
    "    progress=False,\n",
    ")\n",
    "# Merge trained output params with optimized test latents\n",
    "test_pars_iter = {**fixed_pars_iter, \"latents\": test_result_iter.params[\"latents\"]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions on test set from both methods\n",
    "pred_svi = model.predict_outputs(test_pars_svi[\"latents\"], test_pars_svi)\n",
    "pred_iter = model.predict_outputs(test_pars_iter[\"latents\"], test_pars_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "pt_style = {\"ls\": \"none\", \"ms\": 2.0, \"alpha\": 0.5, \"marker\": \"o\", \"color\": \"k\"}\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(10, 10), layout=\"constrained\")\n",
    "\n",
    "# Top row: SVI predictions vs true\n",
    "for i in range(2):\n",
    "    axes[0, i].plot(pred_svi[\"label\"][:, i], test_data[\"label\"].data[:, i], **pt_style)\n",
    "    axes[0, i].set(xlabel=f\"Predicted label {i}\", ylabel=f\"True label {i}\")\n",
    "    axes[0, i].axline([0, 0], slope=1, color=\"tab:green\", zorder=-100)\n",
    "axes[0, 0].set_title(\"SVI: Label 0\")\n",
    "axes[0, 1].set_title(\"SVI: Label 1\")\n",
    "\n",
    "# Bottom row: Iterative predictions vs true\n",
    "for i in range(2):\n",
    "    axes[1, i].plot(pred_iter[\"label\"][:, i], test_data[\"label\"].data[:, i], **pt_style)\n",
    "    axes[1, i].set(xlabel=f\"Predicted label {i}\", ylabel=f\"True label {i}\")\n",
    "    axes[1, i].axline([0, 0], slope=1, color=\"tab:green\", zorder=-100)\n",
    "axes[1, 0].set_title(\"Iterative: Label 0\")\n",
    "axes[1, 1].set_title(\"Iterative: Label 1\")\n",
    "\n",
    "fig.suptitle(\"Test Set: Predicted vs. True Labels\", fontsize=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "Both the SVI and iterative methods visually seem to predict the test set labels, but the iterative optimization appears to yield slightly better accuracy for this toy example."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Here's another way to look at the test set prediction accuracy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute prediction errors on test set\n",
    "svi_label_rmse = np.sqrt(np.mean((pred_svi[\"label\"] - test_data[\"label\"].data) ** 2))\n",
    "iter_label_rmse = np.sqrt(np.mean((pred_iter[\"label\"] - test_data[\"label\"].data) ** 2))\n",
    "\n",
    "svi_flux_rmse = np.sqrt(np.mean((pred_svi[\"flux\"] - test_data[\"flux\"].data) ** 2))\n",
    "iter_flux_rmse = np.sqrt(np.mean((pred_iter[\"flux\"] - test_data[\"flux\"].data) ** 2))\n",
    "\n",
    "print(\"Test Set Performance Comparison\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"{'Method':<20} {'Time (s)':<12} {'Label RMSE':<15} {'Flux RMSE':<15}\")\n",
    "print(\"-\" * 50)\n",
    "print(\n",
    "    f\"{'SVI (10k steps)':<20} {svi_time:<12.2f} {svi_label_rmse:<15.4f} {svi_flux_rmse:<15.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"{'Iterative':<20} {iterative_time:<12.2f} {iter_label_rmse:<15.4f} {iter_flux_rmse:<15.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "## When to use iterative optimization\n",
    "\n",
    "The iterative optimization approach works well when your model is purely linear and should out-perform gradient-based methods in terms of speed, convergence, and often on prediction accuracy as well, given the closed-form solutions available for linear least squares problems. \n",
    "\n",
    "For models with non-linear transforms (e.g., neural networks, Gaussian processes), you should use the standard `optimize()` method with gradient-based optimization."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pollux",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
